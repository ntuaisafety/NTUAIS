---
title: "BlueDot Fellowship 2026: AI Safety Technical Course"
description: "A ten-week guided course on Technical AI Safety, following the BlueDot Impact curriculum."
summary: "Join us for a deep dive into 'The Alignment Problem'. Learn about the drivers of AI progress, threat pathways, and defence strategies."
date: 2026-02-28
tags: ["fellowship", "introductory", "bluedot", "upcoming", "technical"]
badge: "Beginner Friendly"
badge_color: "green"
icon_path: "M12 14l9-5-9-5-9 5 9 5z M12 14l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14z M12 14l9-5-9-5-9 5 9 5zm0 0l6.16-3.422a12.083 12.083 0 01.665 6.479A11.952 11.952 0 0012 20.055a11.952 11.952 0 00-6.824-2.998 12.078 12.078 0 01.665-6.479L12 14zm-4 6v-7.5l4-2.222"
---

**Course Material**: [Technical AI Safety Curriculum](https://bluedot.org/courses/technical-ai-safety)

The **BlueDot Fellowship 2026** at NTUAIS will follow the **BlueDot Impact "Technical AI Safety" curriculum**. This course focuses on the question: **“How do we make AI go well?”**

## Course Overview

On this course, we will identify the future we're working toward and understand the key dynamics of AI safety:

* **Drivers of AI progress**: compute, data, algorithms
* **Threat pathways**: power concentration, gradual disempowerment, catastrophic pandemics, critical infrastructure collapse
* **Plans for making AI go well**: government control over AGI, hand over control to aligned superintelligence, build defences and diffuse AI
* **Layers of defences to build**: prevent dangerous AI actions → constrain dangerous AI capabilities → withstand dangerous AI actions

This course focuses on **defining what AI systems we are building and how**.

## Learning Outcomes

You will gain the technical foundation to understand what it will actually take to make AI systems safer – and why it’s so challenging.

Throughout the course, you will:

* **Diagnose** why making AI safe is technically challenging
* **Evaluate** current safety techniques: what works, what doesn’t, where the gaps are
* **Build** your own "kill chain" showing how defences might break
* **Identify** the most promising intervention point for your contribution
* **Leave** with a fundable action plan to start shipping

## What this course isn't

Though important for making AI go well, we’ll cover the following in separate tracks or courses:

* **AI policy details**: though you'll gain the technical grounding for effective AI governance
* **Compute governance**: hardware verification and tracking deserve their own deep dive
* **AI security**: e.g. preventing model theft or escape
* **ML basics**: please ensure you have a basic understanding of ML before joining (or complete AI foundations modules first)

## Join Us

Let’s start with the question: **“How might we build safe AI?”**

**Start Date:** TBD to Late February 2026
**Format:** Weekly reading + Discussion sessions
