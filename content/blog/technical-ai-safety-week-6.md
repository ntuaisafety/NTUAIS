---
title: "技術性 AI 安全課程：第六週 - 你的下一步"
date: 2026-02-08T22:15:00+08:00
draft: false
summary: "你正站在 AI 安全技術旅程的起點。本週課程整理了 AI 安全技術的全貌，並提供具體的職涯建議與行動計畫，協助你找到適合自己的貢獻方式。"
tags: ["AI Safety", "Course", "Career"]
categories: ["Education"]
author: "Yu-Chen Tsai (Zen)"
showAuthor: false
authors:
  - "zen"
---

**你正站在 AI 安全技術旅程的起點。**

這門課為你整理了 AI 安全技術的全貌：讓 AI 更安全背後的主要挑戰、現有的安全技術，以及它們的不足之處。

你已經開始培養一些**研究品味**——也就是判斷哪些 AI 安全技術領域值得投入、以及為什麼的能力。據我們估計，全職投入讓 AI 系統更安全的人不到 2,000 人，這個領域需要具備 [**高能動性**](https://usefulfictions.substack.com/p/how-to-be-more-agentic) 的人——願意在艱難時仍採取大膽行動。

沒有明確路線圖、指示或先知告訴我們該怎麼做——只有深切在意的人，做著他們相信能把我們帶往更好未來的事。你不需要對 AI 安全有完美的 [內部視角](https://www.neelnanda.io/blog/47-inside-views) 才能開始。這會在你實際嘗試的過程中發展。在此同時，可以依賴你尊重的組織提出的開放問題與提案。

許多人寫過關於 AI 安全技術職涯的好建議，例如 [80,000 Hours](https://80000hours.org/career-reviews/ai-safety-researcher/)、[Marius Hobbhahn](https://www.alignmentforum.org/posts/kpmaEevZ2KehZo2tp/some-advice-on-independent-research)（Apollo 執行長與創辦人）、[Neel Nanda](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher)（Google DeepMind 機詮釋研究負責人）。這裡把那些建議整理成你在這門課之後可以採取的具體行動。

我們最常給大家的建議是：

**1：有疑問時，就試試看！**

如果你不確定自己適不適合某個職位（而且嘗試成本不算高），就去申請。如果你不確定自己有沒有能力完成某個專案，給自己設個時間限制，動手做做看。

（但要注意：沒想清楚就做難以逆轉的事要格外小心，例如在治理領域高調發聲。複現論文並在 Substack 發文通常比較安全。）

**2：在公開場合持續建設**

獲得回饋、讓別人覺得你認真、找到合作夥伴，其中一個很好的方式，就是把你正在做的事告訴大家！你的想法一直放在 Google 文件裡，對你幫助不大。

在 LinkedIn 發文、開一個 Substack、參加 [AI 安全活動](https://luma.com/bluedotevents?k=c)，跟人聊聊你在做的事。沒有人期待你完美。

**3：別等別人許可**

不要假設已經有人在處理你在乎的問題。

就算有，多一雙手幾乎永遠是好事。工作遠多於人，他們總會需要具備合適技能、又有意願投入的人。

**4：考慮「被忽略程度」。**

AI 安全之所以需要更多人投入，一個關鍵原因是：相較於它對世界的重要性，投入的人實在太少。同樣地，在 AI 安全內部，有些領域與組織獲得較多關注，有些較少。

雖然較多人走過的路可能累積更好的職涯資本、也最容易看見，但也想想：你在哪裡有特別的洞察或知識，而投入的人又比較少？

**5：做那些 [不易被「銀河腦」合理化](https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html) 的事。**

人常常會很輕易地合理化自己的行動（例如加入推進 AI 能力的前沿實驗室，好「在決策發生的現場」）。但退一步想、問問自己：同樣的論點是不是也能拿來合理化幾乎任何事——這樣做往往有幫助；並且，一般而言，追求那些簡單、穩健、下行風險不極端高的改變理論。

---

多數人可能會覺得從以下幾大方向著手很有幫助：

## **研究**

> 實作讓 AI 更安全的技術。

這可能包含直接訓練模型、從其他領域引進想法、或更偏理論的工作。例如，「模式生物」的概念最初來自生物學，「紅隊」的做法則來自資安。

對 ML 有深入理解非常加分，但 ML 經驗較少的人，可以靠 [在自己小領域裡做到最強](https://www.lesswrong.com/posts/XvN2QQpKTuEzgkZHY/being-the-pareto-best-in-the-world) 來補。

本週你可以：

- 申請 [Technical AI Safety Project](https://bluedot.org/courses/technical-ai-safety-project) 衝刺，複現並延伸一項有意思的 AI 安全發現。
- 從 [Open Philanthropy](https://coefficientgiving.org/tais-rfp-research-areas/)、[Redwood Research](https://blog.redwoodresearch.org/p/recent-redwood-research-project-proposals)、[UK AISI](https://www.aisi.gov.uk/research-agenda)、[AISI's Alignment Team](https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda) 或 [Anthropic](https://alignment.anthropic.com/2025/recommended-directions/) 等組織挑一個開放問題開始動手
- 獨自完成技術提升計畫，例如 [ARENA](https://www.arena.education/)（或在 Slack 找夥伴！）
- 申請 [MATS](https://www.matsprogram.org/)、[Pivotal](https://www.pivotal-research.org/fellowship)、[LASR](https://www.lasrlabs.org/)、[PIBBS](https://princint.ai/) 或 [Anthropic Fellows 計畫](https://alignment.anthropic.com/2024/anthropic-fellows-program/) 等，在有導師指導下累積研究經驗

## **工程**

> 打造能讓研究者更有效跑實驗的工具與框架。

例如 UK AISI 的 [Inspect](https://inspect.aisi.org.uk/) 和 Anthropic 的 [Petri](https://www.anthropic.com/research/petri-open-source-auditing)。雖然做研究問題看起來更「炫」，工程師透過讓更多人做更有影響力的研究，貢獻可以很大。例如「評估研究」就高度依賴工程與維運。

有扎實的軟體工程背景、甚至產品經驗，有助於開發這類工具。

本週你可以：

- 申請 [Technical AI Safety Project](https://bluedot.org/courses/technical-ai-safety-project) 衝刺，撰寫或改進安全研究程式碼。
- 選一個開源 AI 安全工具的 issue 來解決
- 做一個評估工具的雛型
- 參加 [Apart](https://apartresearch.com/sprints) 等機構的駭客松

## **創立**

> 發起能讓 AI 系統更安全的計畫。

可能已經有組織在做你在乎的事、你可以加入，但有時候編制、文化契合、地點等會卡住。有時候，根本沒人在做你特別在乎的那一塊。

有好點子、又有高能動性非常有幫助。你可以 [申請](https://web.miniextensions.com/9UQbEOQ10oTYrqpIOwVS) 我們的 [孵化週](https://bluedot.org/blog/announcing-incubator-week-v2)，或 [Seldon](https://seldonlab.com/)、[5050](https://www.fiftyyears.com/rfs) 等其他管道。

雖然直接碰模型是最直觀的貢獻方式，但若以為那是**唯一**有影響力的方式就錯了。AI 研究者不是單打獨鬥。他們和營運、設計、產品、研究管理、招募、領域專家一起工作。

在上述類別之外，還有很多有影響力的角色。善用你獨特的強項。

---

**注意**：光靠技術工作不足以確保 AI 系統安全。我們還需要：

- **治理機制**，讓所有 AI 開發者都遵守安全規範
- **AI 資安**，防止模型被竊或逃逸
- **社會防禦**，應對仍漏網的傷害（例如 [生物安全](https://bluedot.org/courses/biosecurity)、資安）

我們會在其他課程中涵蓋這些面向。

## 選擇你的關注焦點

在本單元中，你將為如何開始貢獻制定一份行動計畫。這只是個開端。現階段不期待你已有完美答案。

你或許會發現以下我們提供的其他課程特別有幫助：

- [**AGI 策略課程**](https://bluedot.org/courses/agi-strategy) 用於發展你對如何引導 AI 發展軌跡的變革理論。
- [**AI 安全技術專案**](https://bluedot.org/courses/technical-ai-safety-project) 用於建立你的作品集，或測試你適合成為 AI 安全研究員或工程師與否。

你也可以考慮向 80,000 Hours 索取免費的 [一對一職涯諮詢](https://80000hours.org/speak-with-us/?int_campaign=primary-navigation)。

### 資源（35 分鐘）

- [**成為一個真正會做事的人**](https://www.neelnanda.io/blog/become-a-person-who-actually-does-things?utm_source=bluedot-impact)

    你現在有機會負起責任，*就從讓 AI 變得更好開始*。我們非常期待在這段旅程中協助你，並看看你會做出什麼！

    常見的失敗模式是心想「噢，我其實做不到 X」或說「可能已經有人在做的 Y 了」。你很可能可以做 X，而且可能沒人在做 Y！那個人可以是你！

    **Neel Nanda · 2022 · 5 分鐘**

- [**你該做 AI 安全研究／工程專案嗎？**](https://blog.bluedot.org/p/should-you-do-an-ai-safety-research?utm_source=bluedot-impact)

    **Li-Lian Ang · 2025 · 5 分鐘**

- [**軟體工程師指南：在一週內做出你的第一份 AI 安全貢獻**](https://blog.bluedot.org/p/swe-ai-safety-project-guide?utm_source=bluedot-impact)

    若你已決定做一個專案，你會在這裡找到有用的架構。雖然此資源以軟體工程師為對象，其中的指引也較廣泛適用。

    我們正在為其他背景的學習者製作專屬指南。

    **Li-Lian Ang · 2025 · 10 分鐘**

- [**AI 安全技術團隊的人才需求**](https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams?utm_source=bluedot-impact)

    **請讀至「那麼你如何成為 AI 安全專業人士？」一節結尾。**

    根據對 31 位 AI 安全領導者的訪談，MATS 將研究員分為「迭代者」（快速實驗者）、「連結者」（大局理論家）與「放大器」（團隊乘數）。他們也針對各類型提出有用的發展指引。

    **yams 與 Carson Jones · 2024 · 15 分鐘**

### 練習

- **優先選擇單一介入方式**

    根據你在本課程至今所學，挑選一項 AI 安全技術方法。

    我們也在選讀資源中整理了不同組織感到振奮的技術研究議程。

    為協助你排出優先順序，可考量你認為哪種方法能有效因應你在第五單元發展出的威脅。

    [**建立免費帳戶以儲存你的答案**](https://bluedot.org/login?register=true&redirect_to=%2Fcourses%2Ftechnical-ai-safety%2F6%2F2)

- **自行研究**
  - 這項方法的成功長什麼樣子？它如何讓 AI 更安全？
  - 這項方法目前的狀態為何？政府、AI 公司或其他行動者是否已在進行？若沒有，為什麼沒有？
  - 有哪些組織在從事這項工作，是你可以貢獻或加入的？

    請花約 1 小時進行此練習。

    [**建立免費帳戶以儲存你的答案**](https://bluedot.org/login?register=true&redirect_to=%2Fcourses%2Ftechnical-ai-safety%2F6%2F2)

**選讀資源**

- [**Open Philanthropy AI 安全技術徵求計畫：研究領域**](https://www.openphilanthropy.org/tais-rfp-research-areas/?utm_source=bluedot-impact)

    **Open Philanthropy · 2025**

- [**英國 AISI 對齊團隊：研究議程**](https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda?utm_source=bluedot-impact)

    **Hilton 等人 · 2025**

- [**AI 安全技術研究方向建議**](https://alignment.anthropic.com/2025/recommended-directions?utm_source=bluedot-impact)

    **Anthropic 對齊科學團隊 · 2025**

- [**Redwood Research 近期專案提案**](https://blog.redwoodresearch.org/p/recent-redwood-research-project-proposals?utm_source=bluedot-impact)

    由 Redwood 研究員提出、涵蓋多個領域的實證 AI 安全／資安專案。

    **Greenblatt 等人 · 2025**

- [**我是資深軟體工程師。我如何貢獻 AI 安全？**](https://blog.bluedot.org/p/im-an-experienced-swe?utm_source=bluedot-impact)

    **Li-Lian Ang · 2025 · 5 分鐘**

## 擬定計畫

我們不期待這會非常詳盡或完美——把它當作初稿，一個足以幫你起步的版本就好。

### 學習資源（15 分鐘）

- [**個人行動計畫範本**](https://docs.google.com/document/d/1FN37vYcSZZEDyPddnIuEY6Yqg-TBZFN_lLK_Gyp7H2U/copy)

    使用此範本制定你自己的個人行動計畫。若你認為其他範本更適合，我們鼓勵你自由調整！

    **Dewi Erwan · 2025 · 10 分鐘**

- [**獨立研究的一些建議**](https://www.alignmentforum.org/posts/kpmaEevZ2KehZo2tp/some-advice-on-independent-research)

    **Marius Hobbhahn · 2022 · 5 分鐘**

### 練習

- **撰寫你的個人行動計畫**

    請花約 1 小時完成。

    完成後：1）將 Google 文件設為可分享、2）貼到下方、3）在 Slack 分享給你的小組！

    [**建立免費帳號以儲存你的答案**](https://bluedot.org/login?register=true&redirect_to=%2Fcourses%2Ftechnical-ai-safety%2F6%2F3)

**選讀資源**

- [**對齊職涯指南**](https://blog.bluedot.org/p/alignment-careers-guide)

    這篇文章很長，但充滿可付諸行動的建議，有助於你聚焦想培養的技能，或想走的技術對齊長期路徑。建議略過對你較無關的段落。

    **Charlie Rogers-Smith · 2024 · 45 分鐘**

- [**職涯評析：技術型 AI 安全**](https://80000hours.org/career-reviews/ai-safety-researcher/)

    **80000 Hours · 2023**

- [**成為（帕雷托）世界第一**](https://www.lesswrong.com/posts/XvN2QQpKTuEzgkZHY/being-the-pareto-best-in-the-world)

    本文介紹帕雷托前沿的概念。Rob Miles 的置頂留言也將其與比較優勢連結。閱讀時，思考你的專案能讓你站上哪些帕雷托前沿。

    **John Wentworth · 2019 · 5 分鐘**

- [**如何以早期研究者的身份成功：「精實新創」的做法**](https://forum.effectivealtruism.org/posts/jfHPBbYFzCrbdEXXd/how-to-succeed-as-an-early-stage-researcher-the-lean-startup)

    本文主張：在研究生涯早期，你應把自己當成創業者，努力做出顧客（即更廣大的社群）想消費的產品（即研究成果）。

    **Toby Shevlane · 2021 · 10 分鐘**

- [**如何進入對齊／能動性獨立研究**](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency)

    這篇部落格講述一位對齊獨立研究者如何踏入此領域，包含如何找研究點子與取得資助。建議也看留言，特別是 [Steven Byrnes 的置頂留言](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency?commentId=gmaBDo4HLKYdGKyRF) 以獲另一角度的觀點。

    **John Wentworth · 2021 · 15 分鐘**

- [**如何成為機理可詮釋性研究者**](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher)

    **Neel Nanda · 2025**

- [**在 AI 安全研究工程中晉級**](https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering)

    實用指南，列出為邁向機器學習研究工程師角色而累積技能的一些建議步驟。這些對對齊組織的許多職缺都相當適用。

    **Gabe Mukobi · 2022 · 15 分鐘**

## 謝謝你！

我們希望你在這門課程中收穫豐富，也很期待看到你如何為讓 AI 往好的方向發展貢獻一己之力 :)

## 恭喜你完成「技術性 AI 安全」課程！

現在分享你的觀點吧！那些反思若只是躺在習題框裡，起不了什麼作用。把它們分享給你的網絡，讓對話展開吧。
